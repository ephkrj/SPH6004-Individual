{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 6004_MIMICIII_RNN",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ephkrj/SPH6004-Individual/blob/master/Copy_of_6004_MIMICIII_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUnwRz15GuIP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2cfe5ac-d6b4-4d62-b8e0-27748bbecebe"
      },
      "source": [
        "# mount the drive, in order to load the data from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ5S19ANoZIY"
      },
      "source": [
        "import sys\n",
        "sys.path.append('drive/MyDrive/local_py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EskNMtnu3Z9Y"
      },
      "source": [
        "import gc\n",
        "from time import time\n",
        "import os\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pad_sequences as PadSequences\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import Model, Input, load_model #model_from_json\n",
        "from keras.layers import Masking, Flatten, Embedding, Dense, LSTM, TimeDistributed\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import regularizers\n",
        "from keras import optimizers\n",
        "\n",
        "#from sklearn.cross_validation import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, classification_report\n",
        "from sklearn.metrics import recall_score, precision_score\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHoRCYX9AWHz"
      },
      "source": [
        "# folder for tmp files\n",
        "tmp_output_dir = \"drive/MyDrive/mimic_tmp\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVaQ9uSI4Qp9"
      },
      "source": [
        "df_data = pd.read_table(\"drive/MyDrive/time_series.csv\", delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-w5L60R9eIb"
      },
      "source": [
        "cols = ['Time', 'stay_id', 'stay_key', 'hadm_id', 'age', 'gender', 'Heart Rate',\n",
        "       'Respiratory Rate', 'SpO2/SaO2', 'pH', 'Potassium', 'Calcium',\n",
        "       'Glucose', 'Sodium', 'HCO3', 'White Blood Cells', 'Hemoglobin',\n",
        "       'Red Blood Cells', 'Platelet Count', 'Weight', 'Urea Nitrogen',\n",
        "       'Creatinine', 'Blood Pressure', '1 hours urine output',\n",
        "       '6 hours urine output', 'AKI', 'gcs',\n",
        "       'ventilation', 'vasoactive medications', 'sedative medications']\n",
        "df_data['stay_key'] = df_data['stay_id']\n",
        "df_filled = df_data.groupby('stay_id')[cols].ffill().bfill()\n",
        "# df_filled['AKI_hour'] = df_filled.apply(lambda x: x['Time'] if x['AKI'] == 1 else 0, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6apulcJ5rIM"
      },
      "source": [
        "# === tell the patient whether he/she got AKI in the ICU ===\n",
        "# AKI_time = df_filled[df_filled['AKI'] == 1].groupby('stay_key')['AKI_hour'].first()\n",
        "# new = pd.merge(df_filled, AKI_time, left_on=['stay_key'], right_index=True, how='left').drop('AKI_hour_x', axis=1).rename({'AKI_hour_y': 'AKI_time'}, axis=1)\n",
        "# new['time_to_AKI'] = (pd.to_datetime(new['AKI_time']) - pd.to_datetime(new['Time'])) / np.timedelta64(1, 'h')\n",
        "# new = new[~(new['time_to_AKI'] < 0)]\n",
        "# new['AKI_happen'] = new['time_to_AKI'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
        "\n",
        "# === in time ===\n",
        "# in_time = df_filled.groupby('stay_key')[['Time']].first()\n",
        "# df_filled = pd.merge(df_filled, in_time, left_on=['stay_key'], right_index=True, how='left')\n",
        "# df_filled\n",
        "# df_filled['time_since'] = (pd.to_datetime(df_filled['Time_x']) - pd.to_datetime(df_filled['Time_y'])) / np.timedelta64(1, 'h')\n",
        "# stay_ids = df_filled.stay_key.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXoN97L0jlO4"
      },
      "source": [
        "from keras.layers.core import Dense, Reshape, Lambda, RepeatVector, Permute, Flatten\n",
        "from keras.layers import multiply\n",
        "def attention_3d_block(inputs, TIME_STEPS):\n",
        "    \"\"\"\n",
        "    inputs.shape = (batch_size, time_steps, input_dim)\n",
        "    \"\"\" \n",
        "    input_dim = int(inputs.shape[2])\n",
        "    a = Permute((2, 1))(inputs)\n",
        "    a = Reshape((input_dim, TIME_STEPS))(a)\n",
        "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
        "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
        "    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
        "    output_attention_mul = multiply([inputs, a_probs])\n",
        "    return output_attention_mul"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_PQiMlbZZnZ"
      },
      "source": [
        "def return_data(balancer=True, target='AKI',return_cols=False, \n",
        "                tt_split=0.7, val_percentage=0.8,\n",
        "                cross_val=False, mask=False, dataframe=False,\n",
        "                time_steps=14, split=True, pad=True):\n",
        "\n",
        "  \"\"\"\n",
        "  Returns synthetic or real data depending on parameter\n",
        "  Args:\n",
        "  -----\n",
        "      synth_data : synthetic data is False by default\n",
        "      balance : whether or not to balance positive and negative time windows\n",
        "      target : desired target, supports MI, SEPSIS, VANCOMYCIN or a known lab, medication\n",
        "      return_cols : return columns used for this RNN\n",
        "      tt_split : fraction of dataset to use fro training, remaining is used for test\n",
        "      cross_val : parameter that returns entire matrix unsplit and unbalanced for cross val purposes\n",
        "      mask : 24 hour mask, default is False\n",
        "      dataframe : returns dataframe rather than numpy ndarray\n",
        "      time_steps : 14 by default, required for padding\n",
        "      split : creates test train splits\n",
        "      pad : by default is True, will pad to the time_step value\n",
        "  Returns:\n",
        "  -------\n",
        "      Training and validation splits as well as the number of columns for use in RNN\n",
        "  \"\"\"\n",
        "\n",
        "  df = df_filled.rename({'hadm_id': 'HADM_ID'}, axis=1)\n",
        "  df = df.select_dtypes(exclude=['object'])\n",
        "\n",
        "  if pad:\n",
        "    pad_value=0\n",
        "    df = PadSequences.PadSequences().pad(df, 1, time_steps, pad_value=pad_value)\n",
        "    print('There are {0} rows in the df after padding'.format(len(df)))\n",
        "\n",
        "  COLUMNS = list(df.columns)\n",
        "\n",
        "  if return_cols:\n",
        "    return COLUMNS\n",
        "\n",
        "  if dataframe:\n",
        "    return (df[COLUMNS+[target,\"HADM_ID\"]])\n",
        "\n",
        "\n",
        "  MATRIX = df[COLUMNS+[target]].values\n",
        "  MATRIX = MATRIX.reshape(int(MATRIX.shape[0]/time_steps),time_steps,MATRIX.shape[1])\n",
        "\n",
        "  ## note we are creating a second order bool matirx\n",
        "  bool_matrix = (~MATRIX.any(axis=2))\n",
        "  MATRIX[bool_matrix] = np.nan\n",
        "  MATRIX = PadSequences.PadSequences().ZScoreNormalize(MATRIX)\n",
        "  ## restore 3D shape to boolmatrix for consistency\n",
        "  bool_matrix = np.isnan(MATRIX)\n",
        "  MATRIX[bool_matrix] = pad_value\n",
        "\n",
        "  permutation = np.random.permutation(MATRIX.shape[0])\n",
        "  MATRIX = MATRIX[permutation]\n",
        "  bool_matrix = bool_matrix[permutation]\n",
        "\n",
        "  X_MATRIX = MATRIX[:,:,0:-1]\n",
        "  Y_MATRIX = MATRIX[:,:,-1]\n",
        "\n",
        "  x_bool_matrix = bool_matrix[:,:,0:-1]\n",
        "  y_bool_matrix = bool_matrix[:,:,-1]\n",
        "\n",
        "  X_TRAIN = X_MATRIX[0:int(tt_split*X_MATRIX.shape[0]),:,:]\n",
        "  Y_TRAIN = Y_MATRIX[0:int(tt_split*Y_MATRIX.shape[0]),:]\n",
        "  Y_TRAIN = Y_TRAIN.reshape(Y_TRAIN.shape[0], Y_TRAIN.shape[1], 1)\n",
        "\n",
        "  X_VAL = X_MATRIX[int(tt_split*X_MATRIX.shape[0]):int(val_percentage*X_MATRIX.shape[0])]\n",
        "  Y_VAL = Y_MATRIX[int(tt_split*Y_MATRIX.shape[0]):int(val_percentage*Y_MATRIX.shape[0])]\n",
        "  Y_VAL = Y_VAL.reshape(Y_VAL.shape[0], Y_VAL.shape[1], 1)\n",
        "\n",
        "  x_val_boolmat = x_bool_matrix[int(tt_split*x_bool_matrix.shape[0]):int(val_percentage*x_bool_matrix.shape[0])]\n",
        "  y_val_boolmat = y_bool_matrix[int(tt_split*y_bool_matrix.shape[0]):int(val_percentage*y_bool_matrix.shape[0])]\n",
        "  y_val_boolmat = y_val_boolmat.reshape(y_val_boolmat.shape[0],y_val_boolmat.shape[1],1)\n",
        "\n",
        "  X_TEST = X_MATRIX[int(val_percentage*X_MATRIX.shape[0])::]\n",
        "  Y_TEST = Y_MATRIX[int(val_percentage*X_MATRIX.shape[0])::]\n",
        "  Y_TEST = Y_TEST.reshape(Y_TEST.shape[0], Y_TEST.shape[1], 1)\n",
        "\n",
        "  x_test_boolmat = x_bool_matrix[int(val_percentage*x_bool_matrix.shape[0])::]\n",
        "  y_test_boolmat = y_bool_matrix[int(val_percentage*y_bool_matrix.shape[0])::]\n",
        "  y_test_boolmat = y_test_boolmat.reshape(y_test_boolmat.shape[0],y_test_boolmat.shape[1],1)\n",
        "\n",
        "  X_TEST[x_test_boolmat] = pad_value\n",
        "  Y_TEST[y_test_boolmat] = pad_value\n",
        "\n",
        "  if balancer:\n",
        "    TRAIN = np.concatenate([X_TRAIN, Y_TRAIN], axis=2)\n",
        "    print(np.where((TRAIN[:,:,-1] == 1).any(axis=1))[0])\n",
        "    pos_ind = np.unique(np.where((TRAIN[:,:,-1] == 1).any(axis=1))[0])\n",
        "    print(pos_ind)\n",
        "    np.random.shuffle(pos_ind)\n",
        "    neg_ind = np.unique(np.where(~(TRAIN[:,:,-1] == 1).any(axis=1))[0])\n",
        "    print(neg_ind)\n",
        "    np.random.shuffle(neg_ind)\n",
        "    length = min(pos_ind.shape[0], neg_ind.shape[0])\n",
        "    total_ind = np.hstack([pos_ind[0:length], neg_ind[0:length]])\n",
        "    np.random.shuffle(total_ind)\n",
        "    ind = total_ind\n",
        "    if target == 'MI':\n",
        "      ind = pos_ind\n",
        "    else:\n",
        "      ind = total_ind\n",
        "    X_TRAIN = TRAIN[ind,:,0:-1]\n",
        "    Y_TRAIN = TRAIN[ind,:,-1]\n",
        "    Y_TRAIN = Y_TRAIN.reshape(Y_TRAIN.shape[0], Y_TRAIN.shape[1], 1)\n",
        "\n",
        "  no_feature_cols = X_TRAIN.shape[2]\n",
        "\n",
        "  if mask:\n",
        "    print('MASK ACTIVATED')\n",
        "    X_TRAIN = np.concatenate([np.zeros((X_TRAIN.shape[0], 1, X_TRAIN.shape[2])), X_TRAIN[:,1::,::]], axis=1)\n",
        "    X_VAL = np.concatenate([np.zeros((X_VAL.shape[0], 1, X_VAL.shape[2])), X_VAL[:,1::,::]], axis=1)\n",
        "\n",
        "  if cross_val:\n",
        "    return (MATRIX, no_feature_cols)\n",
        "\n",
        "  if split == True:\n",
        "    return (X_TRAIN, X_VAL, Y_TRAIN, Y_VAL, no_feature_cols,\n",
        "            X_TEST, Y_TEST, x_test_boolmat, y_test_boolmat,\n",
        "            x_val_boolmat, y_val_boolmat)\n",
        "\n",
        "  elif split == False:\n",
        "    return (np.concatenate((X_TRAIN,X_VAL), axis=0),\n",
        "            np.concatenate((Y_TRAIN,Y_VAL), axis=0), no_feature_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QutEYoyxy1l1"
      },
      "source": [
        "def build_model(no_feature_cols=None, time_steps=7, output_summary=False):\n",
        "\n",
        "  \"\"\"\n",
        "  Assembles RNN with input from return_data function\n",
        "  Args:\n",
        "  ----\n",
        "  no_feature_cols : The number of features being used AKA matrix rank\n",
        "  time_steps : The number of days in a time block\n",
        "  output_summary : Defaults to False on returning model summary\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  Keras model object\n",
        "  \"\"\"\n",
        "  print(\"time_steps:{0}|no_feature_cols:{1}\".format(time_steps,no_feature_cols))\n",
        "  input_layer = Input(shape=(time_steps, no_feature_cols))\n",
        "  x = attention_3d_block(input_layer, time_steps)\n",
        "  x = Masking(mask_value=0, input_shape=(time_steps, no_feature_cols))(x)\n",
        "  x = LSTM(256, return_sequences=True)(x)\n",
        "  preds = TimeDistributed(Dense(1, activation=\"sigmoid\"))(x)\n",
        "  model = Model(inputs=input_layer, outputs=preds)\n",
        "\n",
        "  RMS = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08)\n",
        "  model.compile(optimizer=RMS, loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "  if output_summary:\n",
        "    model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOOO9VTrYRA_"
      },
      "source": [
        "def train(model_name=\"MIMIC_AKI\", synth_data=False, target='AKI',\n",
        "          balancer=True, predict=False, return_model=False,\n",
        "          n_percentage=1.0, time_steps=14, epochs=10):\n",
        "  \"\"\"\n",
        "  Use Keras model.fit using parameter inputs\n",
        "  Args:\n",
        "  ----\n",
        "  model_name : Parameter used for naming the checkpoint_dir\n",
        "  synth_data : Default to False. Allows you to use synthetic or real data.\n",
        "\n",
        "  Return:\n",
        "  -------\n",
        "  Nonetype. Fits model only.\n",
        "  \"\"\"\n",
        "\n",
        "  f = open('{1}/pickled_objects/X_TRAIN_{0}.txt'.format(target,tmp_output_dir), 'rb')\n",
        "  X_TRAIN = pickle.load(f)\n",
        "  f.close()\n",
        "\n",
        "  f = open('{1}/pickled_objects/Y_TRAIN_{0}.txt'.format(target,tmp_output_dir), 'rb')\n",
        "  Y_TRAIN = pickle.load(f)\n",
        "  f.close()\n",
        "\n",
        "  f = open('{1}/pickled_objects/X_VAL_{0}.txt'.format(target,tmp_output_dir), 'rb')\n",
        "  X_VAL = pickle.load(f)\n",
        "  f.close()\n",
        "\n",
        "  f = open('{1}/pickled_objects/Y_VAL_{0}.txt'.format(target,tmp_output_dir), 'rb')\n",
        "  Y_VAL = pickle.load(f)\n",
        "  f.close()\n",
        "\n",
        "  f = open('{1}/pickled_objects/x_boolmat_val_{0}.txt'.format(target,tmp_output_dir), 'rb')\n",
        "  X_BOOLMAT_VAL = pickle.load(f)\n",
        "  f.close()\n",
        "\n",
        "  f = open('{1}/pickled_objects/y_boolmat_val_{0}.txt'.format(target,tmp_output_dir), 'rb')\n",
        "  Y_BOOLMAT_VAL = pickle.load(f)\n",
        "  f.close()\n",
        "\n",
        "  f = open('{1}/pickled_objects/no_feature_cols_{0}.txt'.format(target,tmp_output_dir), 'rb')\n",
        "  no_feature_cols = pickle.load(f)\n",
        "  f.close()\n",
        "\n",
        "  X_TRAIN = X_TRAIN[0:int(n_percentage*X_TRAIN.shape[0])]\n",
        "  Y_TRAIN = Y_TRAIN[0:int(n_percentage*Y_TRAIN.shape[0])]\n",
        "\n",
        "  #build model\n",
        "  model = build_model(no_feature_cols=no_feature_cols, output_summary=True,\n",
        "                      time_steps=time_steps)\n",
        "\n",
        "  #init callbacks\n",
        "  tb_callback = TensorBoard(log_dir='{2}/logs/{0}_{1}.log'.format(model_name, time,tmp_output_dir),\n",
        "    histogram_freq=0,\n",
        "    write_grads=False,\n",
        "    write_images=True,\n",
        "    write_graph=True)\n",
        "\n",
        "  #Make checkpoint dir and init checkpointer\n",
        "  checkpoint_dir = \"{1}/saved_models/{0}\".format(model_name,tmp_output_dir)\n",
        "\n",
        "  if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "  checkpointer = ModelCheckpoint(\n",
        "    filepath=checkpoint_dir+\"/model.{epoch:02d}-{val_loss:.2f}.hdf5\",\n",
        "    monitor='val_loss',\n",
        "    verbose=0,\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    mode='auto',\n",
        "    period=1)\n",
        "\n",
        "  #fit\n",
        "  model.fit(x=X_TRAIN,y=Y_TRAIN,batch_size=16,epochs=epochs,\n",
        "    callbacks=[tb_callback], # checkpointer],\n",
        "    validation_data=(X_VAL, Y_VAL),\n",
        "    shuffle=True)\n",
        "\n",
        "  model.save('{1}/saved_models/{0}.h5'.format(model_name,tmp_output_dir))\n",
        "\n",
        "  if predict:\n",
        "    print('TARGET: {0}'.format(target))\n",
        "    Y_PRED = model.predict(X_VAL)\n",
        "    Y_PRED = Y_PRED[~Y_BOOLMAT_VAL]\n",
        "    np.unique(Y_PRED)\n",
        "    Y_VAL = Y_VAL[~Y_BOOLMAT_VAL]\n",
        "    Y_PRED_TRAIN = model.predict(X_TRAIN)\n",
        "    print('Confusion Matrix Validation')\n",
        "    print(confusion_matrix(Y_VAL, np.around(Y_PRED)))\n",
        "    print('Validation Accuracy')\n",
        "    print(accuracy_score(Y_VAL, np.around(Y_PRED)))\n",
        "    print('ROC AUC SCORE VAL')\n",
        "    print(roc_auc_score(Y_VAL, Y_PRED))\n",
        "    print('CLASSIFICATION REPORT VAL')\n",
        "    print(classification_report(Y_VAL, np.around(Y_PRED)))\n",
        "\n",
        "  if return_model:\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Craek_KZ2YV_"
      },
      "source": [
        "def return_loaded_model(model_name=\"mimic_AKI\"):\n",
        "\n",
        "  loaded_model = load_model(\"{1}/saved_models/{0}.h5\".format(model_name,tmp_output_dir))\n",
        "  return loaded_model\n",
        "\n",
        "def pickle_objects(target='AKI', time_steps=14):\n",
        "\n",
        "  (X_TRAIN, X_VAL, Y_TRAIN, Y_VAL, no_feature_cols,\n",
        "   X_TEST, Y_TEST, x_boolmat_test, y_boolmat_test,\n",
        "   x_boolmat_val, y_boolmat_val) = return_data(balancer=True, target=target,\n",
        "                                                            pad=True,\n",
        "                                                            split=True,\n",
        "                                                      time_steps=time_steps)\n",
        "\n",
        "  features = return_data(return_cols=True, target=target, pad=True, split=True,\n",
        "                         time_steps=time_steps)\n",
        "\n",
        "  f = open('{1}/pickled_objects/X_TRAIN_{0}.txt'.format(target,tmp_output_dir), 'wb')\n",
        "  pickle.dump(X_TRAIN, f)\n",
        "  f.close()\n",
        "\n",
        "  f = open('{1}/pickled_objects/X_VAL_{0}.txt'.format(target,tmp_output_dir), 'wb')\n",
        "  pickle.dump(X_VAL, f)\n",
        "  f.close()\n",
        "\n",
        "  f = open('{1}/pickled_objects/Y_TRAIN_{0}.txt'.format(target,tmp_output_dir), 'wb')\n",
        "  pickle.dump(Y_TRAIN, f)\n",
        "  f.close()\n",
        "\n",
        "  f = open('{1}/pickled_objects/Y_VAL_{0}.txt'.format(target,tmp_output_dir), 'wb')\n",
        "  pickle.dump(Y_VAL, f)\n",
        "  f.close()\n",
        "\n",
        "  f = open('{1}/pickled_objects/X_TEST_{0}.txt'.format(target,tmp_output_dir), 'wb')\n",
        "  pickle.dump(X_TEST, f)\n",
        "  f.close()\n",
        "\n",
        "  f = open('{1}/pickled_objects/Y_TEST_{0}.txt'.format(target,tmp_output_dir), 'wb')\n",
        "  pickle.dump(Y_TEST, f)\n",
        "  f.close()\n",
        "\n",
        "  f = open('{1}/pickled_objects/x_boolmat_test_{0}.txt'.format(target,tmp_output_dir), 'wb')\n",
        "  pickle.dump(x_boolmat_test, f)\n",
        "  f.close()\n",
        "\n",
        "  f = open('{1}/pickled_objects/y_boolmat_test_{0}.txt'.format(target,tmp_output_dir), 'wb')\n",
        "  pickle.dump(y_boolmat_test, f)\n",
        "  f.close()\n",
        "\n",
        "  f = open('{1}/pickled_objects/x_boolmat_val_{0}.txt'.format(target,tmp_output_dir), 'wb')\n",
        "  pickle.dump(x_boolmat_val, f)\n",
        "  f.close()\n",
        "\n",
        "  f = open('{1}/pickled_objects/y_boolmat_val_{0}.txt'.format(target,tmp_output_dir), 'wb')\n",
        "  pickle.dump(y_boolmat_val, f)\n",
        "  f.close()\n",
        "\n",
        "  f = open('{1}/pickled_objects/no_feature_cols_{0}.txt'.format(target,tmp_output_dir), 'wb')\n",
        "  pickle.dump(no_feature_cols, f)\n",
        "  f.close()\n",
        "\n",
        "  f = open('{1}/pickled_objects/features_{0}.txt'.format(target,tmp_output_dir), 'wb')\n",
        "  pickle.dump(features, f)\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA_U3-OYHTuj",
        "outputId": "e83f8357-20d6-4639-e11b-3e2456cc8e7c"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    pickle_objects(target='AKI', time_steps=14)#\n",
        "    K.clear_session() #useful when you're creating multiple models in succession, such as during hyperparameter search or cross-validation. \n",
        "\n",
        "    train(model_name='mimic_AKI', epochs=13,\n",
        "          synth_data=False, predict=True, target='AKI', time_steps=14)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 512092 rows in the df after padding\n",
            "(36578, 14)\n",
            "(36578, 14, 28)\n",
            "(28,)\n",
            "(28,)\n",
            "(36578, 14, 28)\n",
            "(36578, 14, 28)\n",
            "(36578, 14, 1)\n",
            "[    7     8    10 ... 25594 25596 25598]\n",
            "[    7     8    10 ... 25594 25596 25598]\n",
            "[    0     1     2 ... 25601 25602 25603]\n",
            "There are 512092 rows in the df after padding\n",
            "time_steps:14|no_feature_cols:28\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 14, 28)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute (Permute)               (None, 28, 14)       0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 28, 14)       0           permute[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 28, 14)       210         reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_vec (Permute)         (None, 14, 28)       0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 14, 28)       0           input_1[0][0]                    \n",
            "                                                                 attention_vec[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "masking (Masking)               (None, 14, 28)       0           multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 14, 256)      291840      masking[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 14, 1)        257         lstm[0][0]                       \n",
            "==================================================================================================\n",
            "Total params: 292,307\n",
            "Trainable params: 292,307\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/13\n",
            "621/621 [==============================] - 11s 13ms/step - loss: 0.1458 - acc: 0.9543 - val_loss: 0.0420 - val_acc: 0.9870\n",
            "Epoch 2/13\n",
            "621/621 [==============================] - 6s 9ms/step - loss: 0.0565 - acc: 0.9780 - val_loss: 0.0065 - val_acc: 0.9984\n",
            "Epoch 3/13\n",
            "621/621 [==============================] - 6s 10ms/step - loss: 0.0101 - acc: 0.9976 - val_loss: 0.0028 - val_acc: 0.9995\n",
            "Epoch 4/13\n",
            "621/621 [==============================] - 6s 9ms/step - loss: 0.0066 - acc: 0.9988 - val_loss: 0.0020 - val_acc: 0.9996\n",
            "Epoch 5/13\n",
            "621/621 [==============================] - 6s 10ms/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.0027 - val_acc: 0.9996\n",
            "Epoch 6/13\n",
            "621/621 [==============================] - 6s 10ms/step - loss: 0.0058 - acc: 0.9989 - val_loss: 0.0014 - val_acc: 0.9998\n",
            "Epoch 7/13\n",
            "621/621 [==============================] - 6s 10ms/step - loss: 0.0035 - acc: 0.9994 - val_loss: 0.0018 - val_acc: 0.9997\n",
            "Epoch 8/13\n",
            "621/621 [==============================] - 6s 10ms/step - loss: 0.0039 - acc: 0.9993 - val_loss: 8.3905e-04 - val_acc: 0.9999\n",
            "Epoch 9/13\n",
            "621/621 [==============================] - 6s 10ms/step - loss: 0.0012 - acc: 0.9997 - val_loss: 6.8893e-04 - val_acc: 0.9998\n",
            "Epoch 10/13\n",
            "621/621 [==============================] - 6s 10ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 8.9429e-04 - val_acc: 0.9998\n",
            "Epoch 11/13\n",
            "621/621 [==============================] - 6s 10ms/step - loss: 0.0016 - acc: 0.9997 - val_loss: 7.6879e-04 - val_acc: 0.9998\n",
            "Epoch 12/13\n",
            "621/621 [==============================] - 6s 9ms/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.0017 - val_acc: 0.9998\n",
            "Epoch 13/13\n",
            "621/621 [==============================] - 6s 10ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0013 - val_acc: 0.9998\n",
            "TARGET: AKI\n",
            "Confusion Matrix Validation\n",
            "[[50075     1]\n",
            " [   10   733]]\n",
            "Validation Accuracy\n",
            "0.9997835455243118\n",
            "ROC AUC SCORE VAL\n",
            "0.9997700937374652\n",
            "CLASSIFICATION REPORT VAL\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00     50076\n",
            "         1.0       1.00      0.99      0.99       743\n",
            "\n",
            "    accuracy                           1.00     50819\n",
            "   macro avg       1.00      0.99      1.00     50819\n",
            "weighted avg       1.00      1.00      1.00     50819\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}